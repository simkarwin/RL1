{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPdY3y1fyBMrcPlOd8O1niF",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dsaldana/reinforcement-learning-course/blob/main/lab6_nonlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 6: Non-linear function approximation\n",
    "\n",
    "## Exercise 1: Q-Learning with a Neural Network (PyTorch) on MountainCar\n",
    "\n",
    "**Objective:**\n",
    "Implement **Q-learning** with a **PyTorch neural network** to solve `MountainCar-v0`. You will approximate Q(s, a) with a small MLP, train it from batches of transitions sampled from a replay buffer, and evaluate the learned policy.\n",
    "\n",
    "---\n",
    "\n",
    "## Environment\n",
    "- **Gym** environment: `MountainCar-v0`\n",
    "- **State**: continuous (position, velocity) → shape `(2,)`\n",
    "- **Actions**: {0: left, 1: no push, 2: right}\n",
    "- **Reward**: -1 per step until the goal (`position >= 0.5`)\n",
    "- **Episode limit**: 500 steps\n",
    "- **Goal**: reduce steps-to-goal and improve return over training\n",
    "\n",
    "---\n",
    "\n",
    "## What You Must Implement\n",
    "\n",
    "### 1) Q-Network (PyTorch)\n",
    "Create a small MLP `QNetwork` that maps `state -> Q-values for 3 actions`.\n",
    "- Inputs: `(batch_size, 2)` float32\n",
    "- Outputs: `(batch_size, 3)` Q-values\n",
    "- Suggested architecture: `2 → 64 → 3` with ReLU\n",
    "- Initialize weights reasonably (PyTorch defaults are fine)\n",
    "\n",
    "### 2) Replay Buffer\n",
    "A cyclic buffer to store transitions `(s, a, r, s_next, done)`:\n",
    "- `append(s, a, r, s_next, done)`\n",
    "- `sample(batch_size)` → tensors ready for PyTorch (float32 for states, int64 for actions, float32 for rewards/done)\n",
    "\n",
    "### 3) ε-Greedy Policy\n",
    "- With probability `epsilon`: pick a random action\n",
    "- Otherwise: `argmax_a Q(s, a)` from the current network\n",
    "- Use **decaying ε** (e.g., from 1.0 down to 0.05 over ~20–50k steps)\n",
    "\n",
    "### 4) Q-Learning Target and Loss\n",
    "For a sampled batch:\n",
    "- Compute `q_pred = Q(s).gather(1, a)`  (shape `(batch, 1)`)\n",
    "- Compute target:\n",
    "  - If `done`: `target = r`\n",
    "  - Else: `target = r + gamma * max_a' Q(s_next, a').detach()`\n",
    "- Loss: Mean Squared Error (MSE) between `q_pred` and `target`\n",
    "\n",
    "> **Stabilization (recommended)**: Use a **target network** `Q_target` (periodically copy weights from `Q_online`) to compute the max over next-state actions. Update every `target_update_freq` steps.\n",
    "\n",
    "### 5) Deep Q-learning method\n",
    "- For each environment step:\n",
    "  1. Select action with ε-greedy\n",
    "  2. Step the env, store transition in buffer\n",
    "  3. If `len(buffer) >= batch_size`:\n",
    "     - Sample a batch\n",
    "     - Compute `q_pred`, `target`\n",
    "     - Backprop: `optimizer.zero_grad(); loss.backward(); optimizer.step()`\n",
    "     - (Optional) gradient clipping (e.g., `clip_grad_norm_` at 10)\n",
    "  4. Periodically update `Q_target ← Q_online` (if using target net)\n",
    "- Track episode returns (sum of rewards) and steps-to-goal\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation\n",
    "- Run **evaluation episodes** with `epsilon = 0.0` (greedy) every N training episodes\n",
    "- Report:\n",
    "  - Average steps-to-goal (lower is better; random policy is ~200)\n",
    "  - Average return (less negative is better)\n",
    "- Plot:\n",
    "  - Training episode return\n",
    "\n",
    "---\n",
    "\n",
    "## Deliverables\n",
    "1. **Code**: In a notebook.\n",
    "2. **Plots**:\n",
    "   - Episode  vs return\n",
    "   - Final value function (State (postition and velocity) Vs Max(Q(state)))\n",
    "\n",
    "3. **Short write-up** (also in the notebook):\n",
    "   - **Performance of your DQN agent**: How quickly does it learn? Does it reach the goal consistently?\n",
    "   - **Comparison with tile coding**:\n",
    "     - Which representation learns faster?\n",
    "     - Which one is more stable?\n",
    "     - How do the function approximation choices (linear with tiles vs. neural network) affect generalization?\n",
    "     - Did the NN require more tuning (learning rate, ε schedule) compared to tile coding?\n",
    "   - **Insights**: What are the trade-offs between hand-crafted features (tiles) and learned features (neural networks)?\n",
    "\n"
   ],
   "metadata": {
    "id": "DzEu8zQt3_MJ"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DHYJhmAv355q",
    "ExecuteTime": {
     "end_time": "2025-10-10T20:52:56.144047Z",
     "start_time": "2025-10-10T20:52:56.141132Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "alpha = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 5000\n",
    "batch_size = 64\n",
    "replay_buffer_size = 50000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T20:52:56.219543Z",
     "start_time": "2025-10-10T20:52:56.217654Z"
    }
   },
   "cell_type": "code",
   "source": "print(env.observation_space)\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "# Define Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim=2, n_actions=3):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ],
   "metadata": {
    "id": "JhoDESZd60Yu",
    "ExecuteTime": {
     "end_time": "2025-10-10T20:52:56.237535Z",
     "start_time": "2025-10-10T20:52:56.235235Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize Q-network and optimizer\n",
    "q_net = QNetwork(state_dim, n_actions).to(device)\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
    "loss_fn = nn.MSELoss()\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)"
   ],
   "metadata": {
    "id": "erbbkUXL65HM",
    "ExecuteTime": {
     "end_time": "2025-10-10T20:52:56.287064Z",
     "start_time": "2025-10-10T20:52:56.281724Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "def epsilon_greedy(state_loc, epsilon_loc, step, decay_steps=50_000):\n",
    "  ############ TODO ###########\n",
    "  epsilon_loc = epsilon_min + (epsilon_loc - epsilon_min) * np.exp(-1.0 * step / decay_steps)\n",
    "  if random.random() < epsilon_loc:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "  else:\n",
    "        # print(f\"{state_loc=}\")\n",
    "        state = torch.FloatTensor(state_loc).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "          q_values = q_net(state)\n",
    "        q_values = q_values.cpu().numpy()\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(q_values == max_q)[0]\n",
    "        action = np.random.choice(max_actions)\n",
    "        return action  # random tie-breaker\n",
    "        "
   ],
   "metadata": {
    "id": "nk6hESNp7KMk",
    "ExecuteTime": {
     "end_time": "2025-10-10T20:52:56.333133Z",
     "start_time": "2025-10-10T20:52:56.330608Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "def train_dqn():\n",
    "    \"\"\"Train the DQN using experience replay.\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "    q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_values = q_net(next_states).max(1)[0].detach()\n",
    "    targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ],
   "metadata": {
    "id": "hcX56dEL7PZd",
    "ExecuteTime": {
     "end_time": "2025-10-10T20:52:56.380299Z",
     "start_time": "2025-10-10T20:52:56.377859Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "## MAIN Loop ###\n",
    "rewards_dqn = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes), leave=True):\n",
    "  state = env.reset()[0]\n",
    "  total_reward = 0\n",
    "  done = False\n",
    "  counter = 0\n",
    "  while not done:\n",
    "    action = epsilon_greedy(state, epsilon, episode)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    ############ TODO ###########\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    train_dqn()\n",
    "    counter += 1\n",
    "    # if counter % 10000 == 0:\n",
    "    #     print(\"counter:{} action: {}, state: {}\".format(counter, action, state))\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "  rewards_dqn.append(total_reward)\n",
    "print(rewards_dqn)\n",
    "    "
   ],
   "metadata": {
    "id": "kgR0ojdN7RuM",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-10T21:24:00.134846Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.85207963  0.00427269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/5000 [00:19<12:31:11,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.7839436   0.00100684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/5000 [00:30<14:18:40, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.33850166  0.02042399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5000 [00:39<13:10:09,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 1, state: [-0.53426516  0.02073868]\n",
      "counter:20000 action: 0, state: [-0.50851977 -0.05618177]\n",
      "counter:30000 action: 2, state: [-0.7929173  -0.02354778]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/5000 [01:11<16:50:43, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.25937817 -0.00099938]\n",
      "counter:20000 action: 1, state: [-0.8235739   0.03069822]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/5000 [01:28<18:57:00, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 2, state: [-0.44198063  0.03674978]\n",
      "counter:20000 action: 1, state: [-1.1113517 -0.0137649]\n",
      "counter:30000 action: 2, state: [-0.865238   -0.03617394]\n",
      "counter:40000 action: 1, state: [-0.57382125 -0.03504675]\n",
      "counter:50000 action: 0, state: [ 0.10958759 -0.01550075]\n",
      "counter:60000 action: 0, state: [-0.11973941 -0.03275781]\n",
      "counter:70000 action: 2, state: [-0.5268002   0.04954583]\n",
      "counter:80000 action: 1, state: [-0.4606324   0.00826167]\n",
      "counter:90000 action: 2, state: [-0.75415784  0.01915178]\n",
      "counter:100000 action: 2, state: [-1.0120693   0.02323061]\n",
      "counter:110000 action: 0, state: [-0.6045859  0.0038088]\n",
      "counter:120000 action: 0, state: [-0.34988105 -0.01012337]\n",
      "counter:130000 action: 1, state: [-0.9951061  -0.00345748]\n",
      "counter:140000 action: 0, state: [-0.34575647  0.00614384]\n",
      "counter:150000 action: 0, state: [-0.47322103 -0.00869492]\n",
      "counter:160000 action: 2, state: [-0.8335608   0.02953197]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/5000 [03:40<49:52:30, 35.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.86270916 -0.02217313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/5000 [03:55<40:39:43, 29.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 1, state: [-0.5549237 -0.0356425]\n",
      "counter:20000 action: 0, state: [-0.13474472 -0.00277726]\n",
      "counter:30000 action: 2, state: [-0.46978927 -0.04533459]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/5000 [04:18<37:55:25, 27.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 1, state: [-0.30836314  0.01470117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/5000 [04:32<32:13:23, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 2, state: [-0.4407754   0.02048686]\n",
      "counter:20000 action: 2, state: [-0.77185607 -0.01573887]\n",
      "counter:30000 action: 1, state: [-0.09955028  0.02788705]\n",
      "counter:40000 action: 2, state: [-0.6153518   0.01117559]\n",
      "counter:50000 action: 0, state: [-0.26262817 -0.04097274]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/5000 [05:11<39:10:13, 28.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 2, state: [-0.21186958 -0.0021939 ]\n",
      "counter:20000 action: 0, state: [0.09498738 0.00688585]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/5000 [05:32<35:47:17, 25.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 1, state: [-0.83964837 -0.03150315]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/5000 [05:41<28:42:14, 20.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.73760325 -0.01284212]\n",
      "counter:20000 action: 2, state: [-0.4430753   0.04977961]\n",
      "counter:30000 action: 2, state: [-0.43518728  0.01788615]\n",
      "counter:40000 action: 2, state: [-0.09055731  0.03439914]\n",
      "counter:50000 action: 1, state: [-0.18089825 -0.04764877]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/5000 [06:21<36:46:32, 26.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.62539434  0.04510339]\n",
      "counter:20000 action: 0, state: [-1.0767242   0.00690133]\n",
      "counter:30000 action: 1, state: [-1.1912637   0.00549445]\n",
      "counter:40000 action: 1, state: [-0.16722193  0.00208259]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/5000 [06:55<40:03:23, 28.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 2, state: [-0.7777497  -0.03598491]\n",
      "counter:20000 action: 0, state: [-0.03365885 -0.03260805]\n",
      "counter:30000 action: 0, state: [-1.1898193 -0.0112497]\n",
      "counter:40000 action: 2, state: [-0.42459399  0.02359404]\n",
      "counter:50000 action: 1, state: [0.05937864 0.03057848]\n",
      "counter:60000 action: 0, state: [-1.1146674  0.0185374]\n",
      "counter:70000 action: 2, state: [-0.04919336  0.01295729]\n",
      "counter:80000 action: 2, state: [-0.7257457   0.01119223]\n",
      "counter:90000 action: 0, state: [-0.02332597 -0.0265806 ]\n",
      "counter:100000 action: 0, state: [-0.33659244  0.00942899]\n",
      "counter:110000 action: 2, state: [-0.09545782  0.00802757]\n",
      "counter:120000 action: 0, state: [ 0.16781071 -0.00118337]\n",
      "counter:130000 action: 2, state: [-0.4653057  -0.04793753]\n",
      "counter:140000 action: 1, state: [-0.01947564 -0.02213885]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/5000 [08:42<51:19:32, 37.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 1, state: [-0.4717437   0.02698622]\n",
      "counter:20000 action: 2, state: [-1.0707957  -0.01865973]\n",
      "counter:30000 action: 2, state: [-0.7759946  -0.00900601]\n",
      "counter:40000 action: 2, state: [0.0050498  0.01817155]\n",
      "counter:50000 action: 2, state: [ 0.1389362  -0.00589636]\n",
      "counter:60000 action: 2, state: [-0.5180985  0.0009403]\n",
      "counter:70000 action: 1, state: [-0.84735155 -0.00905676]\n",
      "counter:80000 action: 2, state: [-0.872727   -0.01502903]\n",
      "counter:90000 action: 0, state: [-0.15696788  0.02478435]\n",
      "counter:100000 action: 2, state: [ 0.1846653  -0.01047408]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/5000 [09:55<66:14:01, 47.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 1, state: [-0.75135636  0.00368155]\n",
      "counter:20000 action: 0, state: [-0.95236576  0.01051763]\n",
      "counter:30000 action: 0, state: [-0.39565682 -0.01175105]\n",
      "counter:40000 action: 0, state: [-0.90568286  0.03639715]\n",
      "counter:50000 action: 2, state: [-0.7214575   0.00466459]\n",
      "counter:60000 action: 2, state: [-0.5366504  0.018983 ]\n",
      "counter:70000 action: 2, state: [-1.0575769  -0.02183812]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/5000 [10:52<69:55:26, 50.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 1, state: [-5.7442951e-01  2.9197325e-05]\n",
      "counter:20000 action: 1, state: [-0.7819707  -0.01106898]\n",
      "counter:30000 action: 2, state: [-0.02939943  0.01806114]\n",
      "counter:40000 action: 2, state: [-0.22346197 -0.03873277]\n",
      "counter:50000 action: 1, state: [-1.0909132   0.01071665]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/5000 [11:29<64:22:18, 46.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.37602916  0.0041364 ]\n",
      "counter:20000 action: 0, state: [-0.7176912  -0.04593456]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/5000 [11:49<53:07:55, 38.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 0, state: [-0.09983114  0.01987481]\n",
      "counter:20000 action: 2, state: [-0.39292136  0.00458989]\n",
      "counter:30000 action: 2, state: [-0.77569467 -0.00466855]\n",
      "counter:40000 action: 1, state: [-0.8167772  -0.00291322]\n",
      "counter:50000 action: 0, state: [-0.5721887  -0.00704318]\n",
      "counter:60000 action: 0, state: [-0.28906298 -0.00406782]\n",
      "counter:70000 action: 0, state: [-0.8820956  0.0089087]\n",
      "counter:80000 action: 1, state: [-0.46572807 -0.01721222]\n",
      "counter:90000 action: 2, state: [-0.6997325   0.02370249]\n",
      "counter:100000 action: 1, state: [0.00717327 0.00561611]\n",
      "counter:110000 action: 2, state: [-0.5523971   0.00578409]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/5000 [13:18<52:16:58, 37.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 1, state: [-0.86378384 -0.00609018]\n",
      "counter:20000 action: 2, state: [-0.49559706 -0.04551506]\n",
      "counter:30000 action: 1, state: [-0.8836343  -0.02108216]\n",
      "counter:40000 action: 2, state: [-0.6244045  -0.02410686]\n",
      "counter:50000 action: 1, state: [-0.83733255 -0.0377605 ]\n",
      "counter:60000 action: 1, state: [-0.96939087  0.00965233]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/5000 [14:05<56:14:58, 40.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 2, state: [-0.256402   -0.00905593]\n",
      "counter:20000 action: 1, state: [-0.8264896   0.01776727]\n",
      "counter:30000 action: 1, state: [-0.49634823  0.02053277]\n",
      "counter:40000 action: 0, state: [-0.24511603  0.03956933]\n",
      "counter:50000 action: 0, state: [-0.44171923 -0.04105083]\n",
      "counter:60000 action: 2, state: [-0.952287   -0.02437901]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/5000 [14:51<58:34:30, 42.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:10000 action: 2, state: [-0.48189497 -0.02584482]\n",
      "counter:20000 action: 1, state: [-0.6213671  -0.00903586]\n",
      "counter:30000 action: 2, state: [-0.220986    0.04594935]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2: Deep Q-Learning (DQN) on LunarLander-v2\n",
    "\n",
    "## Problem Description\n",
    "In this exercise, you will implement **Deep Q-Learning (DQN)** to solve the classic control problem **LunarLander-v2** in Gym.\n",
    "\n",
    "### The Task\n",
    "The agent controls a lander that starts at the top of the screen and must safely land on the landing pad between two flags.\n",
    "\n",
    "- **State space**: Continuous vector of 8 variables, including:\n",
    "  - Position (x, y)\n",
    "  - Velocity (x_dot, y_dot)\n",
    "  - Angle and angular velocity\n",
    "  - Left/right leg contact indicators\n",
    "- **Action space**: Discrete, 4 actions\n",
    "  - 0: do nothing\n",
    "  - 1: fire left orientation engine\n",
    "  - 2: fire main engine\n",
    "  - 3: fire right orientation engine\n",
    "- **Rewards**:\n",
    "  - +100 to +140 for successful landing\n",
    "  - -100 for crashing\n",
    "  - Small negative reward for firing engines (fuel cost)\n",
    "  - Episode ends when lander crashes or comes to rest\n",
    "\n",
    "The goal is to train an agent that lands successfully **most of the time**.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm: Deep Q-Learning\n",
    "You will implement a **DQN agent** with the following components:\n",
    "\n",
    "1. **Q-Network**\n",
    "   - Neural network that approximates Q(s, a).\n",
    "   - Input: state vector (8 floats).\n",
    "   - Output: Q-values for 4 actions.\n",
    "   - Suggested architecture: 2 hidden layers with 128 neurons each, ReLU activation.\n",
    "\n",
    "2. **Target Network**\n",
    "   - A copy of the Q-network that is updated less frequently (e.g., every 1000 steps).\n",
    "   - Used for stable target computation.\n",
    "\n",
    "3. **Replay Buffer**\n",
    "   - Stores transitions `(s, a, r, s_next, done)`.\n",
    "   - Sample random mini-batches to break correlation between consecutive samples.\n",
    "\n",
    "4. **ε-Greedy Policy**\n",
    "   - With probability ε, take a random action.\n",
    "   - Otherwise, take `argmax_a Q(s, a)`.\n",
    "   - Decay ε over time (e.g., from 1.0 → 0.05).\n",
    "\n",
    "5. **Q-Learning Method**\n",
    "   \n",
    "\n",
    "\n",
    "**Final note:**\n",
    "   No code base is necessary. At this point, you must know how to implement evertything.\n",
    "   For reference, but not recommended ([Here](https://colab.research.google.com/drive/1Gl0kuln79A__hgf2a-_-mwoGISXQDK_X?authuser=1#scrollTo=8Sd0q9DG8Rt8&line=56&uniqifier=1) is a solution)\n",
    "\n",
    "---\n",
    "## Deliverables\n",
    "1. **Code**:\n",
    "- Q-network (PyTorch).\n",
    "- Training loop with ε-greedy policy, target network, and Adam optimizer.\n",
    "\n",
    "2. **Plots**:\n",
    "- Episode returns vs training episodes.\n",
    "- Evaluation performance with a greedy policy (ε = 0).\n",
    "\n",
    "3. **Short Write-up (≤1 page)**:\n",
    "- Did your agent learn to land consistently?  \n",
    "- How many episodes did it take before you saw improvement?  \n",
    "- What effect did replay buffer size, target update frequency, and learning rate have on stability?  \n",
    "- Compare results across different runs (does it sometimes fail to converge?).\n",
    "\n",
    "Compare this task with the **MountainCar-v0** problem you solved earlier:\n",
    "- What is **extra** or more challenging in LunarLander?  \n",
    "- Consider state dimensionality, number of actions, reward shaping, and the difficulty of exploration.  \n",
    "- Why might DQN be necessary here, whereas simpler methods (like tile coding) could work for MountainCar?\n"
   ],
   "metadata": {
    "id": "8Sd0q9DG8Rt8"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "J6IXyZqR7zia"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
